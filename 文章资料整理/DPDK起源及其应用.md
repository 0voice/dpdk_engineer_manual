# DPDK起源及其应用

## 1.概述

### 1.1 问题背景

ETSI NFV 技术通过运行在通用 x86 架构硬件上的虚拟化网络功能，为电信运营商和互
联网服务商提供了一种灵活的业务部署手段和高效的组网方案，可以支持固移网络和 IDC中 NAT、 DPI、 EPC、 FW 等各类业务功能的广域灵活部署与网元弹性扩展。
不同于典型数据中心业务和企业网业务，电信广域网业务要求网元（如 BNG、 DPI 等具有高吞吐、低时延、海量流表支持、用户级 QoS 控制的特点。大量实践表明，通用 x86服务器作为 NFV 基础设施用于高转发业务时，面临着严重的转发性能瓶颈，需要有针对性地从硬件架构、系统 I/O、操作系统、虚拟化层、组网与流量调度、 VNF 功能等层面进行性能优化，才能达到各类 NFV 网络业务的高性能转发要求。根据 ETSI 的 NFV 参考架构，现实中的 NFV 应用系统一般由 NFV 基础设施和 VNF 两类系
统服务商提供。因此，相应的 NFV 端到端性能测试，也应划分为底层的 NFV 基础设施性能与上层的 VNF 性能两类，以明确各自的性能瓶颈，并避免性能调优工作相互干扰。在各类 NFV 基础设施性能优化技术方案中， DPDK（ Data Plane Development Kit）类软件加速方案已成为一种普遍采用的基本方法，它以用户数据 I/O 通道优化为基础，结合了Intel VT 技术、操作系统、虚拟化层与 vSwitch 等多种优化方案，已经形成了完善的性能加速整体架构，并提供了用户态 API 供高速转发类应用访问。
在 DPDK 技术的应用中，大多数 NFV 应用与开发单位一般关注如下几类问题：
\1) DPDK 应用于不同硬件、软件环境下的基本性能
\2) DPDK 转发性能调优方法、性能影响因素
\3) DPDK 调优参考配置
\4) DPDK 应用开发的稳定性、可移植性
针对这些问题，本文基于 DPDK 提供 NFV 转发性能调优的最佳实践和测试结果，以有助于业界同仁开展 NFV 系统的研发与性能评估工作。

### 1.2 范围

本白皮书主要集中分析基于 DPDK 的单节点物理主机与虚拟机环境的 I/O 性能优化方法，以及相关性能指标与测试方法。有关多节点 NFV 服务器、端到端 NFV 基础设施、 VNF 相关性能的集成测试方法，请参见 ETS NFV GS-PER-001。
另外，我们使用 DPDK 网站提供的 L2/L3 转发样例程序作为性能测试的环回工具，具体的 VNF 性能调优方法不在本文讨论范围。

## 2.DPDK 技术简介

DPDK 技术框架可以划分为 DPDK 基本技术与 DPDK 优化技术两部分，前者指标准的 DPDK数据平面开发包和 I/O 转发实现技术，本章将概述该部分的主要原理；后者是在 DPDK 应用过程中，为进一步提高各类用户应用程序的转发性能，所采取的性能调优方法和关键配置，

### 2.1 技术原理与架构

由于采用软件转发和软件交换技术，单服务器内部的转发能力是 NFV 系统的主要性能瓶颈。 在各类高速转发的 NFV 应用中，数据报文从网卡中接收，再传送到虚拟化的用户态应用程序（ VNF）处理，整个过程要经历 CPU 中断处理、虚拟化 I/O 与地址映射转换、虚拟交换层、网络协议栈、内核上下文切换、内存拷贝等多个费时的 CPU 操作和 I/O 处理环节。业内通常采用消除海量中断、旁路内核协议栈、减少内存拷贝、 CPU 多核任务分担、 IntelVT 等技术来综合提升服务器数据平面的报文处理性能，普通用户较难掌握。业界迫切需要一种综合的性能优化方案，同时提供良好的用户开发和商业集成环境， DPDK 加速技术方成为其中的典型代表DPDK 是一个开源的数据平面开发工具集，提供了一个用户空间下的高效数据包处理库函数，它通过环境抽象层旁路内核协议栈、 轮询模式的报文无中断收发、优化内存/缓冲区/队列管理、基于网卡多队列和流识别的负载均衡等多项技术，实现了在 x86 处理器架构下的高性能报文转发能力，用户可以在 Linux 用户态空间开发各类高速转发应用，也适合与各类
商业化的数据平面加速解决方案进行集成。英特尔在 2010 年启动了对 DPDK 技术的开源化进程， 于当年 9 月通过 BSD 开源许可协议正式发布源代码软件包，并于 2014 年 4 月在 www.dpdk.org 上正式成立了独立的开源社区平台，为开发者提供支持。开源社区的参与者们大幅推进了 DPDK 的技术创新和快速演进，
而今它已发展成为 SDN 和 NFV 的一项关键技术。



### 2.2 软件架构

DPDK 的组成架构如图 2-1 所示，相关技术原理概述如下： 



![图片](https://mmbiz.qpic.cn/mmbiz_png/69ZU65wcmIn4qQa4kSPk9U5ASOQUfk7c0rUr49pdH78woueic5e3Sn01Qu3HMLvFiaCrgkCN8UfGrHGU4wXwU6qA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在图 2-1 中，在最底部的内核态（ Linux Kernel） DPDK 有两个模块： KNI 与 IGB_UIO。其中， KNI 提供给用户一个使用 Linux 内核态的协议栈，以及传统的 Linux 网络工具（如ethtool, ifconfig）。 IGB_UIO（ igb_uio.ko 和 kni.ko. IGB_UIO）则借助了 UIO 技术，在初始化过程中将网卡硬件寄存器映射到用户态。

如图 2-1， DPDK 的上层用户态由很多库组成，主要包括核心部件库（ Core Libraries）平台相关模块(Platform)、网卡轮询模式驱动模块（ PMD-Natives&Virtual）、 QoS 库、报文转发分类算法（ Classify）等几大类，用户应用程序可以使用这些库进行二次开发，下面分别简要介绍。

核心部件库：该模块构成的运行环境是建立在 Linux 上，通过环境抽象层(EAL)的运行环境进行初始化，包括： HugePage 内存分配、内存/缓冲区/队列分配与无锁操作、 CPU 亲和性绑定等；其次， EAL 实现了对操作系统内核与底层网卡 I/O 操作的屏蔽（ I/O 旁路了内核及其协议栈），为 DPDK 应用程序提供了一组调用接口，通过 UIO 或 VFIO 技术将 PCI 设备地址映射到用户空间，方便了应用程序调用，避免了网络协议栈和内核切换造成的处理延迟。另外，核心部件还包括创建适合报文处理的内存池、缓冲区分配管理、内存拷贝、以及定时器、环形缓冲区管理等。

平台相关模块：其内部模块主要包括 KNI、能耗管理以及 IVSHMEM 接口。其中， KNI 模块主要通过 kni.ko 模块将数据报文从用户态传递给内核态协议栈处理，以便用户进程使用。

传统的 socket 接口对相关报文进行处理；能耗管理则提供了一些 API，应用程序可以根据收包速率动态调整处理器频率或进入处理器的不同休眠状态；另外， IVSHMEM 模块提供了虚拟机与虚拟机之间，或者虚拟机与主机之间的零拷贝共享内存机制，当 DPDK 程序运行时，IVSHMEM 模块会调用核心部件库 API，把几个 HugePage 映射为一个 IVSHMEM 设备池，并通过参数传递给 QEMU，这样，就实现了虚拟机之间的零拷贝内存共享。

轮询模式驱动模块： PMD 相关 API 实现了在轮询方式下进行网卡报文收发，避免了常规报文处理方法中因采用中断方式造成的响应延迟，极大提升了网卡收发性能。此外，该模块还同时支持物理和虚拟化两种网络接口，从仅仅支持 Intel 网卡，发展到支持 Cisco、Broadcom、 Mellanox、 Chelsio 等整个行业生态系统,以及基于 KVM、 VMWARE、 XEN 等虚拟化网络接口的支持。DPDK 还定义了大量 API 来抽象数据平面的转发应用， 如 ACL、 QoS、 流分类和负载均衡等。 并且，除以太网接口外， DPDK 还在定义用于加解密的软硬件加速接口（ Extensions）。

总体而言， DPDK 技术具有如下特征：
采用 BSD License，保证了可合法用于商业产品
支持 RedHat、 CentOS、 Fedora、 Ubuntu 等大多数 Linux 系统，已开始进入主流 Linux

发布版本
DPDK 支持 Run to Completion 和 Pipeline 两种报文处理模式，用户可以依据需求灵活选择，或者混合使用。 Run to Completion 是一种水平调度方式，利用网卡的多队列，将报文分发给多个 CPU 核处理， 每个核均独立处理到达该队列的报文，资源分配相对固定，减少了报文在核间的传递开销，可以随着核的数目灵活扩展处理能力； Pipeline 模式则通过共享环在核间传递数据报文或消息，将系统处理任务分解到不同的 CPU 核上处理， 通过任务分发来减少处理等待时延。 DPDK 的库函数和样例程序十分丰富，包括 L2/L3 转发、 Hash、 ACL、 QoS、环形队
列等大量示例供用户参考。



原文链接：https://mp.weixin.qq.com/s/KzeQogGGRKv_bQKSYTmoSw

原文作者：胡胡子的